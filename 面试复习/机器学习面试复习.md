# 机器学习面试复习

1. SGD，Momentum，Adagard，Adam原理

- SGD：随机梯度下降，每读入一个数据都会立刻计算loss function的梯度来更新参数
- Momentum：在更新方向的时候保留之前的方向，增加稳定性还可摆脱局部最优；如果当前梯度方向与历史梯度一致，则会增强这个方向梯度，如果当前梯度与历史梯度不一致，则梯度会衰减。
- Adagrad：之前的算法，每一个参数都使用相同的学习率；Adagrad可以在训练中自动对学习率进行调整，出现频率较低的参数采用较大的学习率进行更新。
- Adam：利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。

2. L1不可导的时候怎么办？

   损失函数不可导，梯度下降不再有效，可以使用坐标轴下降法，坐标轴下降法是沿着坐标轴的方向进行参数更新。假设有m个特征个数，先固定m-1个值，然后再求另一个的局部最优，从而避免损失函数不可导的问题。[参考链接](https://www.cnblogs.com/ZeroTensor/p/11099332.html)

3. 什么是支持向量机，SVM和LR的区别？

   支持向量机是一个二分类模型，它的基本模型定义为特征空间上的间隔最大的线性分类器，学习策略是最大化分类间隔，最终转化为凸二次规划问题。LR是参数模型，SVM是非参数模型；前者采用logistic loss，后者采用hinge loss。

